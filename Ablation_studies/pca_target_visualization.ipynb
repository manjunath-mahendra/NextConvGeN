{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1795e169-dc12-4db5-bb02-cb9f32396f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: Migraine\n",
      "Processing dataset: Stroke\n",
      "Processing dataset: LiverCirrhosis\n",
      "Processing dataset: LungCancer\n",
      "Processing dataset: ContraceptiveMethods\n",
      "Processing dataset: HeartDisease\n",
      "Processing dataset: HeartFailure\n",
      "Processing dataset: IndianLiverPatients\n",
      "Processing dataset: Obesity\n",
      "Processing dataset: PimaIndianDiabetes\n",
      "All plots have been saved to pca_plots_with_pvalues.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import norm\n",
    "import json\n",
    "\n",
    "# Dataset and model names\n",
    "dataset_names = [\"Migraine\", \"Stroke\", \"LiverCirrhosis\", \"LungCancer\", \"ContraceptiveMethods\", \"HeartDisease\", \"HeartFailure\", \"IndianLiverPatients\", \"Obesity\", \"PimaIndianDiabetes\"]\n",
    "model_names = [\"CTABGAN\", \"CTGAN\", \"NextConvGeN\", \"TabDDPM\", \"CART\", \"DataSynth\", \"GReaT\"]\n",
    "\n",
    "# Paths to the folders\n",
    "prepared_data_path = \"./PreparedData1\"\n",
    "synthetic_data_path = \"./t1_SyntheticData\"\n",
    "\n",
    "# Custom Peacock Test Implementation\n",
    "def peacock_test(sample1, sample2):\n",
    "    \"\"\"\n",
    "    Perform a two-sample Peacock test for multivariate distributions.\n",
    "\n",
    "    Parameters:\n",
    "    - sample1: np.ndarray, shape (n1, d), the first sample of size n1 in d dimensions.\n",
    "    - sample2: np.ndarray, shape (n2, d), the second sample of size n2 in d dimensions.\n",
    "\n",
    "    Returns:\n",
    "    - p_value: float, the p-value for the test.\n",
    "    \"\"\"\n",
    "    n1, d1 = sample1.shape\n",
    "    n2, d2 = sample2.shape\n",
    "\n",
    "    if d1 != d2:\n",
    "        raise ValueError(\"Samples must have the same dimensionality.\")\n",
    "\n",
    "    # Combine the samples\n",
    "    combined_sample = np.vstack((sample1, sample2))\n",
    "    n_combined = combined_sample.shape[0]\n",
    "\n",
    "    # Create ECDF grids\n",
    "    max_values = combined_sample.max(axis=0)\n",
    "    min_values = combined_sample.min(axis=0)\n",
    "    grid = np.linspace(min_values, max_values, n_combined, axis=0)\n",
    "\n",
    "    # Calculate ECDFs\n",
    "    ecdf1 = np.array([np.mean(np.all(sample1 <= point, axis=1)) for point in grid])\n",
    "    ecdf2 = np.array([np.mean(np.all(sample2 <= point, axis=1)) for point in grid])\n",
    "\n",
    "    # Test statistic: maximum absolute difference between the two ECDFs\n",
    "    test_statistic = np.max(np.abs(ecdf1 - ecdf2))\n",
    "\n",
    "    # Calculate p-value\n",
    "    sqrt_term = np.sqrt((n1 * n2) / (n1 + n2))\n",
    "    z_statistic = test_statistic * sqrt_term\n",
    "    p_value = 2 * (1 - norm.cdf(z_statistic))  # Two-sided p-value\n",
    "\n",
    "    return p_value\n",
    "\n",
    "# Function to load and normalize data, removing the target column\n",
    "def load_and_normalize_data(file_path, target_column=None, round_target=False):\n",
    "    data = pd.read_csv(file_path)\n",
    "    if target_column:\n",
    "        data_features = data.drop(columns=[target_column])\n",
    "        target = data[target_column]\n",
    "        if round_target:  # Round target values if specified\n",
    "            target = target.round().astype(int)\n",
    "    else:\n",
    "        data_features = data\n",
    "        target = None\n",
    "    scaler = StandardScaler()\n",
    "    normalized_data = scaler.fit_transform(data_features)\n",
    "    return normalized_data, target\n",
    "\n",
    "# Function to perform PCA and return 2 principal components\n",
    "def perform_pca(data):\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_data = pca.fit_transform(data)\n",
    "    return pca_data\n",
    "\n",
    "# Function to plot PCA data with target labels and p-values\n",
    "def plot_pca_with_target(data, target, title, ax, p_value=None):\n",
    "    scatter = ax.scatter(data[:, 0], data[:, 1], c=target, cmap='viridis', alpha=0.7, s=10)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"PC1\")\n",
    "    ax.set_ylabel(\"PC2\")\n",
    "    ax.set_aspect('equal', adjustable='datalim')\n",
    "    if target is not None:\n",
    "        legend = ax.legend(*scatter.legend_elements(), title=\"Target\")\n",
    "        ax.add_artist(legend)\n",
    "    if p_value is not None:\n",
    "        ax.text(0.05, 0.95, f\"P-value: {p_value:.5f}\", transform=ax.transAxes, fontsize=10, verticalalignment='top')\n",
    "\n",
    "# Save plots to a single PDF\n",
    "pdf_path = \"pca_plots_with_pvalues.pdf\"\n",
    "with PdfPages(pdf_path) as pdf:\n",
    "    for dataset in dataset_names:\n",
    "        print(f\"Processing dataset: {dataset}\")\n",
    "        fig, axes = plt.subplots(1, len(model_names) + 1, figsize=(15, 5))  # Real + synthetic plots\n",
    "\n",
    "        # Load target column name from additional_info.json\n",
    "        additional_info_path = os.path.join(prepared_data_path, dataset, \"supervised\", \"additional_info.json\")\n",
    "        if os.path.exists(additional_info_path):\n",
    "            with open(additional_info_path, 'r') as f:\n",
    "                additional_info = json.load(f)\n",
    "            target_column = additional_info.get(\"target\")[0]\n",
    "        else:\n",
    "            print(f\"Additional info not found for dataset: {dataset}\")\n",
    "            target_column = None\n",
    "\n",
    "        # Load and process real data\n",
    "        real_data_path = os.path.join(prepared_data_path, dataset, \"supervised\", \"training_data.csv\")\n",
    "        if os.path.exists(real_data_path):\n",
    "            real_data, real_target = load_and_normalize_data(real_data_path, target_column)\n",
    "            real_pca = perform_pca(real_data)\n",
    "            plot_pca_with_target(real_pca, real_target, f\"Real Data ({dataset})\", axes[0])\n",
    "        else:\n",
    "            print(f\"Real data not found for dataset: {dataset}\")\n",
    "            continue\n",
    "\n",
    "        # Load and process synthetic data for each model\n",
    "        for i, model in enumerate(model_names):\n",
    "            synthetic_data_path_model = os.path.join(synthetic_data_path, model, dataset, \"supervised\", \"synthetic_data.csv\")\n",
    "            if os.path.exists(synthetic_data_path_model):\n",
    "                synthetic_data, synthetic_target = load_and_normalize_data(synthetic_data_path_model, target_column, round_target=True)\n",
    "                synthetic_pca = perform_pca(synthetic_data)\n",
    "\n",
    "                # Compute p-value using the custom Peacock test\n",
    "                p_value = peacock_test(real_pca, synthetic_pca)\n",
    "\n",
    "                plot_pca_with_target(synthetic_pca, synthetic_target, f\"{model} ({dataset})\", axes[i + 1], p_value)\n",
    "            else:\n",
    "                print(f\"Synthetic data not found for model {model} and dataset {dataset}\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        pdf.savefig(fig, dpi=300)  # Save the current figure to the PDF with high resolution\n",
    "        plt.close(fig)\n",
    "\n",
    "print(f\"All plots have been saved to {pdf_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f86be7b1-3e4e-4e0b-90ce-acac455c81c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: Migraine\n",
      "Processing dataset: Stroke\n",
      "Processing dataset: LiverCirrhosis\n",
      "Processing dataset: LungCancer\n",
      "Processing dataset: ContraceptiveMethods\n",
      "Processing dataset: HeartDisease\n",
      "Processing dataset: HeartFailure\n",
      "Processing dataset: IndianLiverPatients\n",
      "Processing dataset: Obesity\n",
      "Processing dataset: PimaIndianDiabetes\n",
      "All plots have been saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import norm\n",
    "import json\n",
    "\n",
    "# Dataset and model names\n",
    "dataset_names = [\"Migraine\", \"Stroke\", \"LiverCirrhosis\", \"LungCancer\", \"ContraceptiveMethods\", \"HeartDisease\", \"HeartFailure\", \"IndianLiverPatients\", \"Obesity\", \"PimaIndianDiabetes\"]\n",
    "#dataset_names = [\"Migraine\", \"ContraceptiveMethods\"]\n",
    "model_names = [\"CTABGAN\", \"CTGAN\", \"NextConvGeN\", \"TabDDPM\", \"CART\", \"DataSynth\", \"GReaT\"]\n",
    "\n",
    "# Paths to the folders\n",
    "prepared_data_path = \"./PreparedData1\"\n",
    "synthetic_data_path = \"./t1_SyntheticData\"\n",
    "\n",
    "# Custom Peacock Test Implementation\n",
    "def peacock_test(sample1, sample2):\n",
    "    n1, d1 = sample1.shape\n",
    "    n2, d2 = sample2.shape\n",
    "    if d1 != d2:\n",
    "        raise ValueError(\"Samples must have the same dimensionality.\")\n",
    "    combined_sample = np.vstack((sample1, sample2))\n",
    "    n_combined = combined_sample.shape[0]\n",
    "    max_values = combined_sample.max(axis=0)\n",
    "    min_values = combined_sample.min(axis=0)\n",
    "    grid = np.linspace(min_values, max_values, n_combined, axis=0)\n",
    "    ecdf1 = np.array([np.mean(np.all(sample1 <= point, axis=1)) for point in grid])\n",
    "    ecdf2 = np.array([np.mean(np.all(sample2 <= point, axis=1)) for point in grid])\n",
    "    test_statistic = np.max(np.abs(ecdf1 - ecdf2))\n",
    "    sqrt_term = np.sqrt((n1 * n2) / (n1 + n2))\n",
    "    z_statistic = test_statistic * sqrt_term\n",
    "    p_value = 2 * (1 - norm.cdf(z_statistic))\n",
    "    return p_value\n",
    "\n",
    "# Function to load and normalize data, removing the target column\n",
    "def load_and_normalize_data(file_path, target_column=None, round_target=True):\n",
    "    data = pd.read_csv(file_path)\n",
    "    if target_column:\n",
    "        data_features = data.drop(columns=[target_column])\n",
    "        target = data[target_column]\n",
    "        if round_target:  # Round target values if specified\n",
    "            target = target.round().astype(int)\n",
    "    else:\n",
    "        data_features = data\n",
    "        target = None\n",
    "    scaler = StandardScaler()\n",
    "    normalized_data = scaler.fit_transform(data_features)\n",
    "    return normalized_data, target\n",
    "\n",
    "\n",
    "# Function to perform PCA and return 2 principal components\n",
    "def perform_pca(data):\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_data = pca.fit_transform(data)\n",
    "    return pca_data\n",
    "\n",
    "# Function to plot PCA data with target labels and p-values\n",
    "def plot_pca_with_target(data, target, title, save_path, p_value=None):\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))  # Square figure\n",
    "    scatter = ax.scatter(data[:, 0], data[:, 1], c=target, cmap='viridis', alpha=0.7, s=10)\n",
    "    ax.set_title(title, fontsize=18)  # Title font size set to 18\n",
    "    ax.set_xlabel(\"PC1\", fontsize=18)  # X-axis label font size set to 18\n",
    "    ax.set_ylabel(\"PC2\", fontsize=18)  # Y-axis label font size set to 18\n",
    "    ax.set_aspect('equal', adjustable='datalim')\n",
    "    ax.set_xlim(-8, 8)  # Set x-axis limits\n",
    "    ax.set_ylim(-8, 8)  # Set y-axis limits\n",
    "    ax.tick_params(axis='x', labelsize=16)  # Set font size for x-axis ticks\n",
    "    ax.tick_params(axis='y', labelsize=16)  # Set font size for y-axis ticks\n",
    "    if target is not None:\n",
    "        legend = ax.legend(*scatter.legend_elements(), title=\"Target\", fontsize=16, title_fontsize=16, loc= \"upper right\")  # Legend font sizes\n",
    "        ax.add_artist(legend)\n",
    "    if p_value is not None:\n",
    "        ax.text(0.05, 0.95, f\"p-value: {p_value:.5f}\", transform=ax.transAxes, fontsize=16, verticalalignment='top')  # P-value label font size\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)  # Save plot as PNG\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "\n",
    "# Main processing loop\n",
    "for dataset in dataset_names:\n",
    "    print(f\"Processing dataset: {dataset}\")\n",
    "    dataset_dir = os.path.join(\"plots\", dataset)\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "    # Load target column name\n",
    "    additional_info_path = os.path.join(prepared_data_path, dataset, \"supervised\", \"additional_info.json\")\n",
    "    if os.path.exists(additional_info_path):\n",
    "        with open(additional_info_path, 'r') as f:\n",
    "            additional_info = json.load(f)\n",
    "        target_column = additional_info.get(\"target\")[0]\n",
    "    else:\n",
    "        print(f\"Additional info not found for dataset: {dataset}\")\n",
    "        target_column = None\n",
    "\n",
    "    # Process real data\n",
    "    real_data_path = os.path.join(prepared_data_path, dataset, \"supervised\", \"training_data.csv\")\n",
    "    if os.path.exists(real_data_path):\n",
    "        real_data, real_target = load_and_normalize_data(real_data_path, target_column)\n",
    "        real_pca = perform_pca(real_data)\n",
    "        save_path = os.path.join(dataset_dir, f\"Real_{dataset}.png\")\n",
    "        plot_pca_with_target(real_pca, real_target, f\"Real Data ({dataset})\", save_path)\n",
    "    else:\n",
    "        print(f\"Real data not found for dataset: {dataset}\")\n",
    "        continue\n",
    "\n",
    "    # Process synthetic data for each model\n",
    "    for model in model_names:\n",
    "        model_dir = os.path.join(dataset_dir, model)\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        synthetic_data_path_model = os.path.join(synthetic_data_path, model, dataset, \"supervised\", \"synthetic_data.csv\")\n",
    "        if os.path.exists(synthetic_data_path_model):\n",
    "            synthetic_data, synthetic_target = load_and_normalize_data(synthetic_data_path_model, target_column)\n",
    "            synthetic_pca = perform_pca(synthetic_data)\n",
    "            p_value = peacock_test(real_pca, synthetic_pca)\n",
    "            save_path = os.path.join(model_dir, f\"{model}_{dataset}.png\")\n",
    "            plot_pca_with_target(synthetic_pca, synthetic_target, f\"{model}\", save_path, p_value)\n",
    "        else:\n",
    "            print(f\"Synthetic data not found for model {model} and dataset {dataset}\")\n",
    "\n",
    "print(\"All plots have been saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6c6913-61de-494f-8e9b-2f3bcfff8288",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
